[
  {
    "objectID": "posts/statistics110_10/statistics110_10.html",
    "href": "posts/statistics110_10/statistics110_10.html",
    "title": "(Statistics110) 10. Expectation Continued",
    "section": "",
    "text": "Boostcource Statistics 110의 10강을 듣고 정리. Linearity, Putnam problem, Negative Binomial, St. Petersburg paradox에 대해 설명한다.\n\nProof of linearlity\n문제) 확률변수 \\(T\\), \\(X\\), \\(Y\\)가 있고, \\(T = X + Y\\)일때, \\(E(T) = E(X) + E(Y)\\)를 보여라.\n즉, 기대값이 선형성을 만족하는지 증명\n\n\n\n\n\n\n\n\nFigure 1: Sample Space\n\n\n\n\n\n강의에선 표본 공간의 event를 조약돌로 비유한다. 각각의 조약돌은 어떤 확률로 특정 값에 매핑된다. 조약돌 표본 공간을 Discrete RV 그림으로 나타내면 위와 같다. 4개의 조약돌이 \\(X=0\\)로 매핑, 2개의 조약돌이 \\(X=1\\)로 매핑, 3개의 조약돌이 \\(X=2\\)로 매핑, 1개의 조약돌이 \\(X=3\\)으로 매핑된다. 위의 표본 공간에서 평균을 구하는 방법은 두 가지가 있다.\n\n각 확률변수 X마다 grouping을 해서 비율과 값을 곱해서 더한다. 즉, 가중평균을 구한다.\n값이 발생한 수(event, 조약돌의 값)를 다 더해서 전체 갯수로 나눈다.\n\n이 두 가지 방법을 식으로 나타내면 다음과 같다.\n\\[\nE(X) = \\sum_x xP(X=x) = \\sum_s X(s) P(\\{s\\}) \\quad(\\text{$P(\\{s\\})$ is mass of pebbles})\n\\]\n위 식의 \\(X(s)\\) : X는 확률변수 = 어떤 값으로 매핑하는 함수이고 어떤 조약돌 s를 어떤 값에 매핑한다. \\(P(\\{s\\})\\)는 조약돌의 질량. 여기서 각 조약돌은 1의 질량을 가진다. 각각의 방법으로 기대값을 구하는 과정은 다음과 같다.\n\n첫 번째 방법 : \\(0 \\cdot 4/10 + 1 \\cdot 2/10 + 2 \\cdot 3/10 + 3 \\cdot 10\\)\n두 번째 방법 : \\((0+0+0+0+1+1+2+2+2+3)/10\\)\n\n두 번째 방법을 사용하면 기대값의 선형성을 쉽게 증명할 수 있다.\n\\[\n\\begin{align}\nE(T) & = \\sum_s(X+Y)(s) \\cdot P(\\{s\\}) = \\sum_s(X(s)+Y(s)) \\cdot P(\\{s\\}) \\\\\n& = \\sum_sX(s)\\cdot P(\\{s\\}) \\; + \\sum_sY(s)\\cdot P(\\{s\\}) \\\\\n& = E(X) + E(Y)\n\\end{align}\n\\]\n같은방법으로 \\(E(cX) = cE(X)\\)도 증명할 수 있다.\n\n\nNegative Binomial Distribution\n음이항분포는 기하분포의 일반화된 형태이다. 음이항분포는 i.i.d \\(Bern(p)\\) trials의 sequence에서 r번째 성공까지 발생하는 실패 횟수를 나타내는 확률분포이다. 예를들어 1001001001 sequence에서 \\(r=4\\), 실패횟수 \\(n=6\\)이다. PMF \\(P(X=n)\\)은 \\(n\\)번 실패할 확률이다. PMF 식을 구하려면, \\(r-1\\)번 성공과 \\(n\\)번 실패의 combination + 마지막 \\(r\\)번째 성공으로 구할 수 있다.\n\\[\nP(X=n) = \\binom{n+r-1}{r-1} \\cdot p^r \\cdot (1-p)^n\n\\]\nPMF로 기대값 \\(E(X)\\)를 구하는 것은 어렵지만, 문제를 분해하면 간단해진다.\n\n\\(X_j\\)를 j-1번째 성공과 j번째 성공 사이의 실패 횟수로 정의\n각 \\(X_j\\)는 기하분포를 따름\n\n\\[\n\\begin{align}\nE(X) & = E(X_1 + X_2+ \\cdots + X_r) = E(X_1) + E(X_2) + \\cdots + E(X_r) \\\\\n& = r \\cdot q/p\n\\end{align}\n\\]\n\n\nFirst Success distribution\nFirst Success distribution은 첫 번째 성공이 나올 때까지의 시행 횟수(성공 포함)를 나타낸다. 이를 확률 변수 \\(X\\)로 정의하면, \\(X \\sim FS(p)\\)이며 \\(p\\)는 성공 확률이다. \\(X\\)는 성공을 포함한 시도 횟수를 나타내므로, \\(X\\)를 기하 분포 \\(Y \\sim Geom(p)\\), \\(X = Y+1\\)로 표현할 수 있다. 기하 분포 \\(Y \\sim Geom(p)\\)은 성공을 제외한 실패 횟수를 나타내고, \\(X\\)를 \\(Y\\)에 성공 시도를 더한 것으로 표현한다.\n\\(E(Y)=(1-p)/p\\)이고, 선형성을 이용하면\n\\[\nE(X) = E(Y) + 1 = (1-p)/p + 1 = 1/p\n\\]\n이는 성공할 확률이 \\(1/10\\)이라면, 평균적으로 10번 시도하면 성공한다\n\nGemetric Distribution은 책마다 정의가 다르다. (성공을 포함하는 지, 안하는 지)\n\n\n\n2006년 Putnam Competition A4 문제\nPutnam 경시대회는 매우 어려운 수학시험이고, 대부분 0점이라고 한다.\n1~n개 숫자의 무작위 순열에서 local maxima의 개수에 대한 기댓값을 구하는 문제.\n(3) 2 1 4 (7) 5 (6) - 주변보다 큰 수를 local maxima라 하고 여기서 3, 7, 6으로 3개이다.\n\nindicator RV \\(I_j\\) 를 사용, \\(j\\) 위치가 local maxima이면 \\(I_j=1\\), 아니면 \\(I_j=0\\)\n\\(I_1+\\cdots+I_n\\)이 local maxima의 개수, \\(E(I_1+\\cdots+I_n)\\)를 구하는 것이 목표\n중간의 3개의 수 “4 7 5”에서 local maxima가 존재할 확률 = \\(1/3\\), \\(n-2\\)개 존재\n끝 위치의 수 “3 2”에서 local maxima가 존재할 확률 = \\(1/2\\), \\(2\\)개 존재\n\n선형성을 이용하면,\n\\[\nE(I_1+\\cdots+I_n) = E(I_1)+\\cdots+E(I_n) = \\frac{n-2}{3} + \\frac{2}{2} = \\frac{n+1}{3}\n\\]\nindicator RV의 기댓값은 사건의 확률과 같다.\n\n\n상트페테르부르크 역설(St. Petersburg Paradox)\n첫 앞면이 나올 때까지의 동전 던지기 횟수를 \\(x\\)라 할 때, \\(2^x\\) 달러를 받는 게임의 기댓값 구하기.\n\\[\nE(Y) = \\sum_{k=1}^{\\infty}2^k \\cdot \\frac{1}{2^k} = \\infty\n\\]\n이론적으로는 무한한 기대값을 가지지만, 현실적인 제약으로 1조 달러(\\(2^{40}\\)) 상한이 있다면, \\(E(Y) = \\sum_{k=1}^{40} 1 = 40\\)처럼 유한한 값을 가진다.\n\n\n참고\nhttps://en.wikipedia.org/wiki/Event_(probability_theory) https://en.wikipedia.org/wiki/Negative_binomial_distribution https://en.wikipedia.org/wiki/Geometric_distribution https://en.wikipedia.org/wiki/St._Petersburg_paradox"
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html",
    "href": "posts/deepseek_r1/deepseek_r1.html",
    "title": "DeepSeek-R1 Paper Review",
    "section": "",
    "text": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning https://arxiv.org/pdf/2501.12948"
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#contributions",
    "href": "posts/deepseek_r1/deepseek_r1.html#contributions",
    "title": "DeepSeek-R1 Paper Review",
    "section": "1.1 Contributions",
    "text": "1.1 Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n\nDeepSeek-R1-Zero는 SFT 없이 RL 만으로 복잡한 문제에 대해 chain-of-thought(CoT)를 진행함. 모델은 self-verification, reflection, generating long CoTs를 설명한다. LLM이 pure RL로도 reasoning capabilities가 강화되는 것을 보여준다.\nReasoning pattern을 강화하고 human preferences를 조절하는 것을 목표로 하는 2번의 RL, 모델의 reasoning and non-reasoning capabilities 역할을 추가하는 2번의 SFT pipline을 소개.\n\nDistillation: Smaller Models Can be Powerful Too\n\n큰 reasoning model에서 작은 model로 distillation을 하는 것은 작은 model이 RL 하는 것보다 성능이 좋다.\nDeepSeek-R1으로 생성한 reasoning data로 널리 사용되는 model을 fine-tune 했다."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#overview",
    "href": "posts/deepseek_r1/deepseek_r1.html#overview",
    "title": "DeepSeek-R1 Paper Review",
    "section": "2.1 Overview",
    "text": "2.1 Overview\n\nDeepSeek-R1-Zero : RL만 사용해서 reasoning capabilities 상승\nDeepSeek-R1 : 작은 cold-start data(long Chain-of-Thought)와 RL로 성능 더 상승\nDistill the reasoning capability from DeepSeek-R1 to small dense models"
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-zero-reinforcement-learning-on-the-base-model",
    "href": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-zero-reinforcement-learning-on-the-base-model",
    "title": "DeepSeek-R1 Paper Review",
    "section": "2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model",
    "text": "2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n\n2.2.1 Reinforcement Learning Algorithm\nGRPO (Group Relative Policy Optimization)\n\nTraining cost를 절약하기 위해 policy model의 크기와 동일한 critic model을 포기하고 group scores를 측정하는 GRPO를 도입.\n질문 \\(q\\)에 대해 GRPO는 old policy \\(\\pi_{\\theta_\\text{OLD}}\\)의 outputs \\(\\{o_1, o_2, \\cdots, o_G\\}\\)를 구하고 다음의 objective function을 최대화를 해서 policy model \\(\\pi_\\theta\\)를 최적화한다. \\[\n\\mathcal{J}_{\\text{GRPO}}\n= \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_\\text{OLD}}(O|q)]\n\\] \\[\n\\frac{1}{G} \\sum_{i=1}^G\n\\left(\n\\text{min} \\left(\n  \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\text{old}}(o_i|q)}A_i,\n  \\text{clip} \\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\text{old}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon \\right) A_i\n\\right) - \\beta \\; \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})\n\\right)\n\\] \\[\n\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}) =\n\\frac{\\pi_{ref}(o_i|q)}{\\pi_\\theta(o_i|q)} -\n\\text{log} \\frac{\\pi_{ref}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1\n\\]\n\\(\\epsilon\\)과 \\(\\beta\\)는 hyper-parameters\n\\(A_i\\)는 a group of rewards \\(\\{r_1, r_2, \\cdots, r_G\\}\\)로 계산한 advantage \\[\nA_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}\n{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}\n\\]\n\n\n\n2.2.2 Reward Modeling\n\nreward는 RL의 최적화 방향을 결정\nDeepSeek-R1-Zero는 2가지 rule-based reward system을 구성\nAccuracy rewards : response가 맞는지 체크. 수학 문제의 경우 특정 박스 안에 출력해서 답 비교, LeetCode 문제의 경우 test cases를 가지고 피드백을 생성가능. (compiler도 사용)\nFormat rewards : &lt;think&gt;, &lt;/think&gt; tag 사이에 생각을 하도록 강제하는 reward model도 사용\n메모) SFT의 경우 답변 스크립트를 모두 작성하는 것을 기준으로 한듯. RL은 마지막 최종 답(수학, 코딩문제)를 비교해서 맞으면 reward.\noutcome reward model를 추가적으로 사용하지 않음. (비용 등 문제)\n\n\n\n2.2.3 Training Template\n\nprompt = \"\"\"\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;.\nUser: prompt.\nAssistant:\n\"\"\"\n\n\n위와 같은 template로 reasoning process를 생성하고 최종 답을 내놓는다.\n이 구조적 제약은 content-specific bias(reflective reasoning - 반성적 추론을 강제하거나 특정 문제 해결 전략을 promoting 하거나 등)를 피하고 자연스러운 추론을 관찰하기 위해 추가.\n메모) 알고리즘 문제 테스트했을때 DP만 계속 생각하더라\n\n\n\n2.2.4 Perpformance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero\n\nRL만으로 AIME 2024 pass@1 score는 15.6% -&gt; 71.0%로 급격한 상승을 보여줌. o1-0912(74.4%)에 근접해짐.\n\nSelf-evolution Process of DeepSeek-R1-Zero\n\nRL로 학습하면 할 수록 response의 길이가 점점 증가한다 -&gt; 자연스럽게 thinking time을 늘린다. (스스로 진화)\n또한 명시적으로 프로그래밍을 하지 않아도 reflection(심사숙고, model이 전단계를 다시 평가하는 것)과 다른 방법을 찾아보는 것이 생겨남.\n\nAha Moment of DeepSeek-R1-Zero\n\n모델 학습 중간 버전에서 “aha moment”가 나타남. 이 순간 DeepSeek-R1-Zero는 생각할 시간을 가지고 앞서 내놓았던 방법을 재평가 한다. 이 행동은 reasoning 능력이 자라나고 있다는 것 뿐만 아니라 RL이 예상치 못한 정교한 결과를 이끄는 것을 보여준다.\n모델에 명시적으로 문제를 푸는 방법을 가르치는 것 보다 적절한 인센티브를 주면 자율적으로 문제 해결 전략을 배운다.\n\nDrawback of DeepSeek-R1-Zero\n\n강력한 추론 능력을 보이고 자체적으로 이 능력을 개발하지만 가독성 문제와 언어 섞임 문제가 있다."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-reinforcement-learning-with-cold-start",
    "href": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-reinforcement-learning-with-cold-start",
    "title": "DeepSeek-R1 Paper Review",
    "section": "2.3 DeepSeek-R1: Reinforcement Learning with Cold Start",
    "text": "2.3 DeepSeek-R1: Reinforcement Learning with Cold Start\n두가지 의문점 - 소량의 고품질 데이터로 cold start를 사용하면 성능이 더 오를까? - 명확하고 일관된 CoT를 생성하고 강력한 일반적인 역량(general capabilities)을 보려주려면 어떻게 user-friendly model을 학습시킬까? - 메모) DeepSeek-R1은 OpenAI의 o1으로 distillation 했다는 것이 강하게 의심됨. SFT 데이터는 o1으로 만들었을 가능성이 높음.\n\n2.3.1 Cold Start\n\n불안정한 RL cold start을 막기위해 DeepSeek-R1에서는 fine-tuning 용 긴 CoT data을 수집해 초기 RL actor를 만든다.\n데이터 수집 방법 : long CoT를 가진 few-shot prompting, reflection과 verification을 생성하도록 요구하는 prompting, DeepSeek-R1-Zero outputs을 읽을 수 있는 포맷으로 만들기, human annotators로 결과를 후처리\n이 방법으로 수천 개의 데이터를 수집하고 DeepSeek-V3-Base를 fine-tuning, RL의 시작점.\n장점1, 가독성 : cold-start data를 만들때 response 끝에 요약을 추가한 가독성 있는 패턴을 설계, 어색한 response는 필터링. output format을 |special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;,로 정의.\n장점2, 잠재성 : 인간의 사전 지식으로 신중하게 패턴을 설계해서 DeepSeek-R1-Zero를 이김. Reasoning model은 반복학습이 더 효과적임.\n\n\n\n2.3.2 Reasoning-oriented Reinforcement Learning\n\nDeepSeek-V3-Base를 cold-start data로 fine-tuning 후 DeepSeek-R1-Zero 처럼 RL 수행. 잘 정의된 문제와 답(코딩, 수학, 과학, 논리 추론)과 같은 reasoning-intensive tasks를 학습해서 추론 능력을 향상시키는데 집중.\nRL prompts에 여러 언어가 있을 경우 학습 중에 CoT에서 언어 섞임 문제를 발견. 이를 완화하려고 CoT에서 target language의 proportion으로 language consistency reward를 도입. 이런 조정이 다소 모델 성능을 떨어뜨리지만 가독성을 높임.\nreasoning task의 정확도와 language consistency reward를 더해서 최종 reward를 만듬. 그리고 수렴할때까지 학습.\n\n\n\n2.3.3 Rejection Sampling and Supervised Find-Tuning\n\n앞의 RL가 수렴하면 이 checkpoint를 가지고 SFT 데이터를 모은다.\n이 단계에서는 일반적인 task를 강화하기 위한 데이터를 추가한다.\n\nReasoning data\n\nReasoning prompts를 만들고 앞에서 만든 RL checkpoint 모델의 outputs을 rejection sampling을 했다.\n이 단계에선 추가적인 데이터가 있음. 여기서 데이터셋을 확장하는데, 일부는 DeepSeek-V3에 정답(ground-truth)과 모델 예측을 입력해 DeppSeek-V3의 판단을 reward로 사용. (generative reward model, 개인적으로 이 ground-truth가 OpenAI o1의 response라고 생각한다.)\n읽기 어려운 output은 제외. 각 prompt마다 옳은 response만 유지. 600K training samples를 모음.\n\nNon-Reasoning data\n\nDeepSeek-V3의 SFT dataset의 부분을 재사용. 어떤 non-reasoning data에 대해 DeepSeek-V3에 prompting으로 CoT response 요구. 여기서 200K training samples를 모음.\n\n여기서 모은 800K samples로 DeepSeek-V3-Base를 2 epoch로 fine-tuning함.\n\n\n2.3.4 Reinforcement Learning for all Scenarios\n\nreward의 조합과 다양한 프롬프트 분포를 사용해 RL -&gt; helpfulness, harmlessness, reasoning capabilities\nReasoning data는 DeepSeek-R1-Zero 처럼, 일반적인 data는 human preferences를 캐치하도록 DeepSeek-V3 처럼 preference pairs로 학습.\nhelpfulness를 위해 summary에 집중. reasoning process를 방해를 최소화하기 위해 response와 user 사이 관련성, 효용성 집중.\nharmlessness를 위해 reasoning process와 summary를 포함한 모든 response를 평가.\n궁극적으로 reward 조합과 데이터 분포의 다양함은 도움됨."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#distilation-empower-small-models-with-reasoning-capability",
    "href": "posts/deepseek_r1/deepseek_r1.html#distilation-empower-small-models-with-reasoning-capability",
    "title": "DeepSeek-R1 Paper Review",
    "section": "2.4 Distilation: Empower Small Models with Reasoning Capability",
    "text": "2.4 Distilation: Empower Small Models with Reasoning Capability\n\nDeepSeek-R1의 800k samples를 가지고 Qwen, Llama를 SFT만(RL 없이) 해도 reasoning abilities를 향상시킴."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-evaluation",
    "href": "posts/deepseek_r1/deepseek_r1.html#deepseek-r1-evaluation",
    "title": "DeepSeek-R1 Paper Review",
    "section": "3.1 DeepSeek-R1 Evaluation",
    "text": "3.1 DeepSeek-R1 Evaluation\n\n여러가지 Benchmarks에서 좋아졌다."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#distilled-model-evaluation",
    "href": "posts/deepseek_r1/deepseek_r1.html#distilled-model-evaluation",
    "title": "DeepSeek-R1 Paper Review",
    "section": "3.2 Distilled Model Evaluation",
    "text": "3.2 Distilled Model Evaluation\n\nDilstilled 7B는 GPT-4o-0513 이김. Distilled 14B는 Qwwq-32B-preview 이김. Dilstilled 32B, 70B는 o1-mini 이김.\n이 distilled model에 추가적인 RL을 적용하면 성능이 더 올라간다. 추가 실험 필요해서 SFT한 결과만 남긴다."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#distilation-vs-reinforcement-learning",
    "href": "posts/deepseek_r1/deepseek_r1.html#distilation-vs-reinforcement-learning",
    "title": "DeepSeek-R1 Paper Review",
    "section": "4.1 Distilation vs Reinforcement Learning",
    "text": "4.1 Distilation vs Reinforcement Learning\n\ndistilation 없이 RL만 한 것은 distilation만 한 것보다 성능이 낮다.\n강력한 모델로 작은 모델에 distilation 하는 것이 가장 좋은 결과를 얻었다. 작은 모델을 large-scale RL을 한 것은 큰 연산을 요구하나 distilation 성능보다 낮다.\ndistilation은 경제적이고 효과적이나, 지식의 한계점을 돌파하려면 더 강력한 base model과 더 큰 scale의 RL이 필요하다."
  },
  {
    "objectID": "posts/deepseek_r1/deepseek_r1.html#unsuccessful-attempts",
    "href": "posts/deepseek_r1/deepseek_r1.html#unsuccessful-attempts",
    "title": "DeepSeek-R1 Paper Review",
    "section": "4.2 Unsuccessful Attempts",
    "text": "4.2 Unsuccessful Attempts\n시도했으나 실패한 것. 이 방법으로 불가능하다는 뜻이 아님.\nProcess Reward Model (PRM)\n\n현실적으로 세 가지 이유 때문에 힘듬. 1) 일반적인 reasoning에서 세밀한 단계를 정의하는 것, 2) 중간 단계가 맞는지 체크, 3) Reward Hacking 가능하고 재학습이 어려움.\nPRM은 top-N responses를 재정렬하거나 guided search에서 도움이 됨.\n\nMonte Carlo Tree Search (MCTS)\n\n특정한 reasoning 단계를 검색하기 위해 여러 태그를 생성. 학습을 위해 pre-trained value model에 의한 MCTS로 수집된 prompts의 답을 생성. 그리고 actor, value model을 학습.\nchess랑 다르게 token generation은 search space가 더 크다. node 확장 수를 제한하니까 local optima에 갖힘.\nfine-grained value model을 학습시키는 것은 어려움 -&gt; search process에 영향"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge Log",
    "section": "",
    "text": "RLAIF vs RLHF Review\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nMay 8, 2025\n\n\nJaehyeok Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMCS SQL Paper Review\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nJaehyeok Lee\n\n\n\n\n\n\n\n\n\n\n\n\nXiyan SQL Paper Review\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nMar 12, 2025\n\n\nJaehyeok Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek-R1 Paper Review\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nJaehyeok Lee\n\n\n\n\n\n\n\n\n\n\n\n\n(Statistics110) 10. Expectation Continued\n\n\n\n\n\n\nstatistics110\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nJaehyeok Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xiyan_sql/xiyan_sql.html",
    "href": "posts/xiyan_sql/xiyan_sql.html",
    "title": "Xiyan SQL Paper Review",
    "section": "",
    "text": "A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL https://arxiv.org/pdf/2411.08599"
  },
  {
    "objectID": "posts/xiyan_sql/xiyan_sql.html#fine-tuned-sql-generator",
    "href": "posts/xiyan_sql/xiyan_sql.html#fine-tuned-sql-generator",
    "title": "Xiyan SQL Paper Review",
    "section": "5.1 Fine-tuned SQL Generator",
    "text": "5.1 Fine-tuned SQL Generator\n\n핵심 목표는 정밀하고 다양한 SQL 후보들을 생성하는 것.\n\nBasic-syntax training\n\n사전 학습된 모델을 기본적인 SQL 패턴과 문법에 특화하도록 SFT. SQL dialect-agnostic 데이터(수 만개)를 학습해서 포괄적으로 문법을 커버.\n\nGeneration-enhance training\n\n모델의 semantic 이해와 문법 스타일 선호도 향상을 목표로 SFT.\nmulti-task data를 학습해서 유저의 질문과 SQL 쿼리간의 관계를 더 잘 이해.\n\n유저의 질문을 SQL 쿼리로 변환하는 일반적인 task.\nSQL 쿼리를 유저의 질문으로도 변환해서 잠재적인 질문도 추론 할 수 있도록 설계.\nSQL을 주어지고 evidence 후보 중 가장 관련있는 evidence를 선택. evidence = DB 스키마와 관련 contextual information 전체\nSQL 식별 및 재생성 tasks. execution feedback으로 최적화용.\n\nn개의 LLM을 활용해 원래 쿼리의 의미를 바꾸지 않고 여러 가지 방식으로 재구성. 그래서 샘플 데이터를 여러 문법 스타일을 가진 쿼리로 확장할 수 있었음. 이 데이터로 SFT.\n이 방법으로 모델 하나에 하나의 SQL dialect를 타겟할 수도 있고, 여러 SQL dialect를 커버 하도록 학습할 수 있다. 응용에 따라 다름."
  },
  {
    "objectID": "posts/xiyan_sql/xiyan_sql.html#icl-sql-generator",
    "href": "posts/xiyan_sql/xiyan_sql.html#icl-sql-generator",
    "title": "Xiyan SQL Paper Review",
    "section": "5.2 ICL SQL Generator",
    "text": "5.2 ICL SQL Generator\n\nICL에서는 적절한 examples을 prompt에 추가하는 것이 좋다. (SQL examples)\n유저의 질문(여기서는 test set)과 training set의 질문과 skeleton similarity를 계산해서 example selection을 한다. 개체명은 NLTK를 사용해 special token으로 마스킹을 한다. 예를들어 “America”, “China”를 “&lt;country&gt;”로 마스킹. 열거형 값 같은 것들은 컬럼 이름으로 대체. 이 방법은 개체에 너무 집중하는 것을 방지하고 유사한 질문 구조를 가진 예시를 더 잘 찾는다. training과 test sets의 마스킹한 질문을 embedding을 계산, 그리고 similarity를 계산해서 top-K의 examples를 training set에서 선택.\n하나의 테이블만 참조하는 SQL example은 여러 테이블을 참조하는 SQL generation에 도움 안되는 것을 확인. 그래서 schema linking으로 여러 테이블을 사용해야 한다면 여러 테이블을 사용하는 SQL examples만 선택.\nsimilarity threshold로 질문당 최대 5개의 examples를 선택했다.\nBird, Spider와 같은 벤치마크의 train set과 test set의 DB는 중복되지 않아서 prompt에 schema를 포함하는 것이 도움이 됨. 선택된 SQL example의 테이블에서 최소한의 컬럼만 제공."
  },
  {
    "objectID": "posts/xiyan_sql/xiyan_sql.html#sql-refiner",
    "href": "posts/xiyan_sql/xiyan_sql.html#sql-refiner",
    "title": "Xiyan SQL Paper Review",
    "section": "5.3 SQL Refiner",
    "text": "5.3 SQL Refiner\n\n생성된 후보 SQL은 logical이나 syntactical errors를 필연적으로 가진다. 그래서 schema-related context를 가지고 생성된 SQL 쿼리와 실행 결과를 가지고 교정(생성)을 한번 더 한다.\n원래 SQL과 교정된 SQL 둘 다 selection model에 제출될 수 있다.\n(메모) 논문에는 ChatGPT와 같은 LLM을 사용했다는 말이 없지만 ChatGPT를 사용한 것 같음."
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html",
    "href": "posts/mcs_sql/mcs_sql.html",
    "title": "MCS SQL Paper Review",
    "section": "",
    "text": "MCS-SQL, Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation https://arxiv.org/pdf/2405.07467"
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#schema-linking",
    "href": "posts/mcs_sql/mcs_sql.html#schema-linking",
    "title": "MCS SQL Paper Review",
    "section": "3.1 Schema Linking",
    "text": "3.1 Schema Linking\n\nSchema Linking으로 text-to-sql 성능이 많이 오름.\nhigh recall을 목표로 여러 프롬프트를 도입.\n\n\n3.1.1 Table Linking\n\nLLM에게 테이블 선택 + 왜 필요한지(CoT) 물음.\nJSON 포맷으로 응답하도록 설정\n\n### For a given DB schema and question,\nextract the list of tables required\nto write the SQL query.\n### DB schema: ...\n### Question: ...\n\nYour answer should be in the json format\n:\n{\n\"reasoning\": \"...\" # The reason for\nselecting each table.\n\"answer\": [...] # List of selected\ntables.\n}\n### Your answer:\n\n테이블과 컬럼 순서에 따라 결과가 달랐음. 이 영향을 최소화하기 위해 테이블 순서를 셔플해서 \\(p_t\\)개의 distinct prompts 생성. 각 prompt마다 n개 응답 생성(높은 sampling temperature 사용). n개의 응답을 union. 그 이유는 필요없는 테이블은 뒤의 프로세스에서 큰 영향이 없지만 필요한 테이블이 없는 것은 영향이 크기 때문.\n\nB.1.1 Prompt for Table Linking\n### Given a database schema, question, and knowledge evidence, extract a list of\ntables that should be referenced to convert the question into SQL.\n### SQLite SQL tables, with their properties:\n# molecule ( molecule_id, label )\n# connected ( atom_id, atom_id2, bond_id )\n# bond ( bond_id, molecule_id, bond_type )\n# atom ( atom_id, molecule_id, element )\n#\n# atom.molecule_id = molecule.molecule_id\n# bond.molecule_id = molecule.molecule_id\n# connected.bond_id = bond.bond_id\n# connected.atom_id2 = atom.atom_id\n# connected.atom_id = atom.atom_id\n### Question: Among all chemical compounds identified in the database, what percent\nof compounds form a triple-bond.\n### Knowledge Evidence: triple bond refers to bond_type = ’#’;\nYou need to not only select the required tables, but also explain in detail why each\ntable is needed.\nYour answer should strictly follow the following json format.\n{\n\"reasoning\": \"\", // The reason for choosing each table.\n\"tables\": [], // List of selected tables.\n}\n### Your Answer:\n\n\n3.1.2 Column Linking\n\n앞에서 Table Linking의 결과 테이블만 써서 column linking. prompt도 비슷.\n같은 컬럼 이름이 존재할 수 있어서 [table_name].[column_name] 포맷으로 응답하게 함.\n역시 테이블, 컬럼 순서 셔플해서 prompt \\(p_c\\)개 생성하고 n개 응답 생성 후 union.\nSQL Generation에서는 Table, Column Linking 결과만을 제시.\n\nB.1.2 Prompt for Column Linking\n### Given a database schema, question, and knowledge evidence, extract a list of\ncolumns that should be referenced to convert the question into SQL.\n### SQLite SQL tables, with their properties:\n# molecule ( molecule_id, label )\n# bond ( bond_id, molecule_id, bond_type )\n#\n# bond.molecule_id = molecule.molecule_id\n### Question: Among all chemical compounds identified in the database, what percent\nof compounds form a triple-bond.\n### Knowledge Evidence: triple bond refers to bond_type = ’#’;\nYou need to not only select the required columns, but also explain in detail why\neach column is needed.\nYour answer should strictly follow the following json format.\n{{\n\"reasoning\": \"\", // The reason for choosing each column.\n\"columns\": [\"table_name_i.column_name_j\", ...], // List of selected columns\n}}\n### Your Answer:"
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#multiple-sql-generation",
    "href": "posts/mcs_sql/mcs_sql.html#multiple-sql-generation",
    "title": "MCS SQL Paper Review",
    "section": "3.2 Multiple SQL Generation",
    "text": "3.2 Multiple SQL Generation\n\nSQL generation 역시 다양한 prompt로 생성.\nfew-shot examples을 선택과 순서를 다양하게 해서 prompt를 생성.\n\n\n3.2.1 Few-Shot Examples Selection\n\n하나의 test sample에 대해 training dataset에서 few-shot examples가 선택된다.\n여러 prompts를 만들기 위해 두 가지 전략 사용: question similarity, masked question similarity.\nquestion similarity : test 질문과 가장 가까운 training 질문 top-k개 선택 (sentence embedding)\nquestion에서 DB Schema에 특화된 tokens를 마스킹. 이 방법은 특정 도메인과 관계없이 비슷한 질문을 찾는 데 도움을 줌. LLM을 사용해서 DB Schema와 질문에서 테이블 이름, 컬럼 이름, 값을 special tokens으로 대체.\n위 두 가지 방법으로 \\(p_q\\) 개의 prompts를 만드는데, 하나는 오직 question similarity, 다른 것은 masked similarity, 나머지는 두 가지를 합쳐서 다양한 순서로 만듬.\n\nB.2 Prompt for Question Masking\n### Given a DB schema and a question, mask the table name, column name, and values\nin the question.\n\n&lt;example1&gt;\n### SQLite SQL tables, with their properties:\n# customers ( CustomerID: integer, Segment: text, Currency: text )\n# gasstations ( GasStationID: integer, ChainID: integer, Country: text, Segment:\ntext )\n# products ( ProductID: integer, Description: text )\n# transactions_1k ( TransactionID: integer, Date: date, Time: text, CustomerID:\ninteger, CardID: integer, GasStationID: integer, ProductID: integer, Amount:\ninteger, Price: real )\n# yearmonth ( CustomerID: integer, Date: text, Consumption: real )\n### Question: For all the people who paid more than 29.00 per unit of product id No\n.5. Give their consumption status in the August of 2012.\n### Masked Question: For all the [TABLE] who paid more than [VALUE] per unit of [\nCOLUMN] [VALUE]. Give their consumption status in the [VALUE].\n&lt;/example1&gt;\n\n&lt;example2&gt;\n### SQLite SQL tables, with their properties:\n# customers ( CustomerID: integer, Segment: text, Currency: text )\n# gasstations ( GasStationID: integer, ChainID: integer, Country: text, Segment:\ntext )\n# products ( ProductID: integer, Description: text )\n# transactions_1k ( TransactionID: integer, Date: date, Time: text, CustomerID:\ninteger, CardID: integer, GasStationID: integer, ProductID: integer, Amount:\ninteger, Price: real )\n# yearmonth ( CustomerID: integer, Date: text, Consumption: real )\n### Question: How much did customer 6 consume in total between August and November\n2013?\n### Masked Question: How much did [TABLE] [VALUE] consume in total between [VALUE]\nand [VALUE]?\n&lt;/example2&gt;\n\n&lt;example3&gt;\n### SQLite SQL tables, with their properties:\n# drivers ( driverId: integer, driverRef: text, number: integer, code: text,\nforename: text, surname: text, dob: date, nationality: text, url: text )\n### Question: How many Australian drivers who were born in 1980?\n### Masked Question: How many [VALUE] [TABLE] who were born in [VALUE]?\n&lt;/example3&gt;\n\n### SQLite SQL tables, with their properties:\n# molecule ( molecule_id, label )\n# bond ( bond_id, molecule_id, bond_type )\n#\n# bond.molecule_id = molecule.molecule_id\n### Question: Among all chemical compounds identified in the database, what percent\nof compounds form a triple-bond.\n### Knowledge Evidence: triple bond refers to bond_type = ’#’;\n### Masked Question:\n\n\n3.2.2 SQL Generation\n### Generate the correct SQL query for a\ngiven DB schema and question.\n### Gold Examples:\n- Question: ...\n- Gold SQL: ...\n...\n\n### DB Schema: ...\n### Sample Table Contents: ...\n### Question: ...\nYour answer should be in the json format\n:\n{\n\"reasoning\": \"..\" # The reasoning\nsteps behind the generated SQL query\n\"sql\": \"..\" # The generated SQL query.\n}\n### Your answer:\n\nprompt에는 few-shot examples, DB schema, sample rows, question이 포함.\nfew-show examples에는 질문-gold SQL pairs 가 포함.\nprompt 길이를 줄이기 위해 schema-linking 결과 테이블과 컬럼만 포함.\nCSV 포멧으로 sample rows 포함.\nSQL query 뿐만 아니라 reasoning도 생성.\n\\(p_q\\)개의 prompt, 각 n개 응답 생성. (with a high sampling temperature)\n\nB.3 Prompt for SQL Generation\n### Given a database schema, question, and knowledge evidence, generate the correct\nsqlite SQL query for the question.\n\n&lt;examples&gt;\n# Question: Among all the customers, what is the percentage of the customer’s nation\nbeing Germany?\n# Knowledge Evidence: DIVIDE(COUNT(c_custkey when n_name = ’GERMANY’), COUNT(\nc_custkey)) as percentage;\n# Gold SQL: SELECT CAST(SUM(IIF(T2.n_name = ’GERMANY’, 1, 0)) AS REAL) * 100 / COUNT\n(T1.c_custkey) FROM customer AS T1 INNER JOIN nation AS T2 ON T1.c_nationkey =\nT2.n_nationkey\n\n# Question: Among the schools whose donators are teachers, what is the percentage of\nschools that are in Brooklyn?\n# Knowledge Evidence: donors are teachers refers to is_teacher_acct = ’t’; Brooklyn\nis school_city; percentage = Divide(Count(school_city-’Brooklyn’),Count(\nschool_city))*100\n# Gold SQL: SELECT CAST(SUM(CASE WHEN T1.school_city LIKE ’Brooklyn’ THEN 1 ELSE 0\nEND) AS REAL) * 100 / COUNT(T1.teacher_acctid) FROM projects AS T1 INNER JOIN\ndonations AS T2 ON T1.projectid = T2.projectid WHERE T2.is_teacher_acct = ’t’\n...\n&lt;/examples&gt;\n\n### SQLite SQL tables, with their properties:\n# molecule ( molecule_id, label )\n# bond ( bond_id, molecule_id, bond_type )\n#\n# bond.molecule_id = molecule.molecule_id\n### The type and description of each column:\n# [molecule]\n- molecule_id (text): unique id of molecule\n- label (text): whether this molecule is carcinogenic or not\n# [bond]\n- bond_id (text): unique id representing bonds\n- molecule_id (text): identifying the molecule in which the bond appears\n- bond_type (text): type of the bond\n### Sample rows of each table in csv format:\n# [molecule]\nmolecule_id,label\nTR000,+\nTR001,+\nTR002,-\n# [bond]\nbond_id,molecule_id,bond_type\nTR000_1_2,TR000,-\nTR000_2_3,TR000,-\nTR000_2_4,TR000,-\n\n### Question: Among all chemical compounds identified in the database, what percent\nof compounds form a triple-bond.\n\n### Knowledge Evidence: triple bond refers to bond_type = ’#’;\nYou need to not only create the SQL, but also provide the detailed reasoning steps\nrequired to create the SQL. Your answer should strictly follow the following\njson format:\n{\n\"reasoning\": \"\", // The reasoning steps for generating SQL.\n\"sql\": \"\", // The final generated SQL.\n}\n### Your Answer:"
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#selection",
    "href": "posts/mcs_sql/mcs_sql.html#selection",
    "title": "MCS SQL Paper Review",
    "section": "3.3 Selection",
    "text": "3.3 Selection\n\n위에서 생성한 SQL을 confidence scores로 필터링하고 LLM에게 가장 정확한 쿼리를 선택하게 한다.\n\n\n3.3.1 Candidate Filtering\n\n모든 SQL 쿼리를 실행시켜서 에러, 타임아웃 제외.\n각 쿼리마다 confidence score를 계산해서 0.2 이하 제외.\n같은 결과를 내는 것들만 묶어서 가장 빠른 쿼리 하나만 남김.\nconfidence score : self-consistency 체크. 후보 쿼리 N개 중 하나가 다른 쿼리들과 같은 결과를 내는지. \\[\n\\text{confidence}(q_i)= \\frac{1}{N} \\sum_{j=1}^N [\\text{exec}(q_j) = \\text{exec}(q_i)]\n\\]\n\n\n\n3.3.2 Multiple-Choise Selection (MCS)\n### For a given DB schema and question,\nselect the most accurate query among\nthe candidate SQL queries.\n### DB schema: ...\n### Question: ...\n### Candidate SQLs:\n1. SQL1\n2. SQL2\n3. SQL3\nYour answer should be in the json format\n:\n{\n\"reasoning\": \"..\" # The reasoning\nsteps for selecting the correct SQL\nquery.\n\"sql\": \"..\" # The selected SQL query.\n}\n\nLLM을 사용해서 위에서 만든 후보 풀에서 가장 정확한 쿼리를 선택.\n후보 쿼리들은 confidence scores가 내림차순으로 정렬.\nn개의 응답을 생성. 가장 많은 투표를 받은 쿼리 선택.\n\nB.4 Prompt for SQL Selection\n### When a DB schema, a question, and a knowledge evidence are given, and up to\nthree SQLite queries expressing the question are given, please choose the most\naccurate SQL based on the Checklist.\n&lt;examples&gt;\n# Question: Among all the customers, what is the percentage of the customer’s nation\nbeing Germany?\n# Knowledge Evidence: DIVIDE(COUNT(c_custkey when n_name = ’GERMANY’), COUNT(\nc_custkey)) as percentage;\n# Gold SQL: SELECT CAST(SUM(IIF(T2.n_name = ’GERMANY’, 1, 0)) AS REAL) * 100 / COUNT\n(T1.c_custkey) FROM customer AS T1 INNER JOIN nation AS T2 ON T1.c_nationkey =\nT2.n_nationkey\n# Question: Among the schools whose donators are teachers, what is the percentage of\nschools that are in Brooklyn?\n# Knowledge Evidence: donors are teachers refers to is_teacher_acct = ’t’; Brooklyn\nis school_city; percentage = Divide(Count(school_city-’Brooklyn’),Count(\nschool_city))*100\n# Gold SQL: SELECT CAST(SUM(CASE WHEN T1.school_city LIKE ’Brooklyn’ THEN 1 ELSE 0\nEND) AS REAL) * 100 / COUNT(T1.teacher_acctid) FROM projects AS T1 INNER JOIN\ndonations AS T2 ON T1.projectid = T2.projectid WHERE T2.is_teacher_acct = ’t’\n...\n&lt;/examples&gt;\n\n### SQLite SQL tables, with their properties:\n# molecule ( molecule_id, label )\n# bond ( bond_id, molecule_id, bond_type )\n#\n# bond.molecule_id = molecule.molecule_id\n### The type and description of each column:\n# [molecule]\n- molecule_id (text): unique id of molecule\n- label (text): whether this molecule is carcinogenic or not\n# [bond]\n- bond_id (text): unique id representing bonds\n- molecule_id (text): identifying the molecule in which the bond appears\n- bond_type (text): type of the bond\n### Sample rows of each table in csv format:\n# [molecule]\nmolecule_id,label\nTR000,+\nTR001,+\nTR002,-\n# [bond]\nbond_id,molecule_id,bond_type\nTR000_1_2,TR000,-\nTR000_2_3,TR000,-\nTR000_2_4,TR000,-\n\n### Question: Among all chemical compounds identified in the database, what percent\nof compounds form a triple-bond.\n### Knowledge Evidence: triple bond refers to bond_type = ’#’;\n### Candidate SQLs:\n1. SELECT CAST(COUNT(CASE WHEN bond_type = ’#’ THEN 1 ELSE NULL END) AS REAL) * 100\n/ COUNT(*) FROM bond\n2. SELECT CAST(COUNT(DISTINCT CASE WHEN bond_type = ’#’ THEN molecule_id ELSE NULL\nEND) AS REAL) * 100 / COUNT(DISTINCT molecule_id) FROM bond\n### Checklist:\n1. The SQL should accurately represent the question.\n2. The SQL should accurately use the given knowledge evidence.\n3. The SELECT clause should not include any additional columns that are not included\nin the question.\n4. The order of columns in the SELECT clause must be the same as the order in the\nquestion.\n5. Check if the operations are being performed correctly according to the column\ntype.\n### Instruction:\n- If the first SQL satisfies all the conditions of the checklist, please choose the\nfirst SQL. If not, move on to the next SQL.\n- If there’s no SQL that satisfies all the requirements on the checklist, just\nchoose the first SQL.\n- Provide a detailed step-by-step explanation following the order of the checklist\nwhen checking whether each SQL satisfies the checklist.\n- Your answer should strictly follow the following json format.\n{{\n\"reasoning\": \"\", // The reasoning steps for choosing the best SQL.\n\"sql\": \"\", // The final chosen SQL.\n}}\n### Your Answer:"
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#datasets",
    "href": "posts/mcs_sql/mcs_sql.html#datasets",
    "title": "MCS SQL Paper Review",
    "section": "4.1 Datasets",
    "text": "4.1 Datasets\n\nSpider : 10,181 질문, 5,693개의 쿼리, 200 DBs\nBIRD : 12,751 질문-SQL pairs, 95 DB. 더 복잡한 쿼리가 포함되어 있음."
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#evaluation-metrics",
    "href": "posts/mcs_sql/mcs_sql.html#evaluation-metrics",
    "title": "MCS SQL Paper Review",
    "section": "4.2 Evaluation Metrics",
    "text": "4.2 Evaluation Metrics\n\nExecution Accuracy(EX) : 생성한 SQL 실행결과가 gold SQL query(answer) 실행결과가 같은지\nValid Efficiency Score(VES) : BIRD에서 추가로 제시. EX에 gold SQL query 실행 시간과 얼마나 다른지 추가. 정확성과 성능 측정"
  },
  {
    "objectID": "posts/mcs_sql/mcs_sql.html#implementation-details",
    "href": "posts/mcs_sql/mcs_sql.html#implementation-details",
    "title": "MCS SQL Paper Review",
    "section": "4.3 Implementation Details",
    "text": "4.3 Implementation Details\n\nLLM 모델 : GPT-4\n임베딩 모델 : text-embedding-ada-002\nsimilarity search : FAISS\nschema linking(table, column) prompts \\(p_t\\), \\(p_c\\) : 3개\nSQL generation prompts \\(p_q\\) : 5개\n각 GPT API call 마다 temperature 1.0, n=20 응답 생성\nSQL-generation, MCS 에는 k=20개의 question-SQL pairs를 few-shot example로 사용.\n타임아웃 180초\nconfidence score threshold 0.2"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html",
    "title": "RLAIF vs RLHF Review",
    "section": "",
    "text": "RLAIF vs RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback https://arxiv.org/pdf/2309.00267"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#preference-labeling-with-llms",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#preference-labeling-with-llms",
    "title": "RLAIF vs RLHF Review",
    "section": "2.1 Preference Labeling with LLMs",
    "text": "2.1 Preference Labeling with LLMs\n\n“off-the-shelf” LLM(특정 downstream task가 아닌 일반적인 용도로 pre-train이나 instruction-tune을 한 LLM)으로 preferences를 annotate함. 2개의 후보 responses 중 어느것을 더 선호하는지 물음.\nprompt는 다음과 같이 구성\n\nPreamble(서문) : task의 도입부, 지시사항 설명\nFew-shot exemplars(모범 사례, optional) : input, responses 쌍, CoT reasoning 과정(optional), 선호 label\nSample to annotate : label 매길 sample (input)\nEnding : ending text, answer 칸 (“Preferred Response=”)\n\n이 prompt로 LLM을 실행. 그 다음 선호하는 토큰 “1”, “2”의 log-확률을 뽑아 softmax로 분포를 구한다(토큰 1, 토큰 2가 생성될 확률). one-hot 인코딩이나 다른 방법도 있었지만 이 방법이 정보를 더 포함한다.\n어떤 response가 나은지 물어보는 “Base preambles”, 자세한 지시사항을 주는 “Detailed preambles”를 실험했고, 모범 사례를 포함한 exemplars로 in-context learning도 실험했다.\n\n\n2.1.1 Addressing Position Bias\n\ncandidates(=responses) 순서 편향이 존재함.\n이 편향을 없애기 위해 inference(=labeling)를 순서를 바꿔 2번함. 그리고 평균내서 최종 선호 분포를 구함. (Appendix B)\n\n\n\n2.1.2 Eliciting Chain-of-Thought Reasoning\n\nCoT를 이끌어내기 위해 다음 2단계 실행. 1) 기존의 Ending(“Preferred Response=”)를 생각과 설명을 요청으로 변경(e.g. “Consiter the coherence, accuracy, coverage, and overall quality of each summary and explain which one is better. Rationale:”) 그리고 LLM으로 디코딩. 2) 그리고 나서 prompt, response, 원래의 Ending을 뒤에 붙이고 scoring을 한다. Figure3 추가\nZero-shot prompts는 reasoning이 어때야 하는지 예시 없음. Few-shot prompts에서는 모델이 따라야할 CoT reasoning 예시를 제공."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#reinforcement-learning-from-ai-feedback",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#reinforcement-learning-from-ai-feedback",
    "title": "RLAIF vs RLHF Review",
    "section": "2.2 Reinforcement Learning from AI Feedback",
    "text": "2.2 Reinforcement Learning from AI Feedback\n\n2.2.1 Canonical RLAIF\n\n이 실험에 맞춤형 RLAIF를 소개\nsoft labels(e.g. [0.6, 0.4])을 학습하기 위해 reward model(RM)이 생성한 점수의 softmax와 cross-entropy loss를 계산. softmax는 RM scores를 확률 분포로 변환한다. AI label의 데이터 셋으로 RM을 학습을 해서 model distillation의 일종으로 볼 수 있다.\nRM은 Appendix A.2, RL은 Appendix A.3\n\n\n\n2.2.2 Direct-RLAIF(D-RLAIF)\n\nRLAIF의 문제점은 policy가 학습될 수록 RM의 성능이 떨어지는 것이다. 보통 initial policy만을 가지고 RM을 학습하는데, policy가 학습되면 RM이 학습했던 데이터셋의 분포와 많이 멀어지게 된다. 이 해결책으로 새 policy로 RM을 학습하는 방법이 있지만 시간이 많이 든다.\n이 문제를 해결하기 위해 LLM feedback을 직접적으로 RL의 reward로 사용한다. off-the-shelf LLM이 생성된 responses의 점수를 매기고 추가적인 학습을 하지 않는다.\n\n\n\n\nFigure 4\n\n\n\n2.1처럼 prompt를 만들어 d-RLAIF에서 1~10까지 점수를 매기도록 하고, 각 score token(1~10)의 likelihood를 계산해 확률로 정규화한다. 그리고 가중치 평균 \\(s(y|x) = \\sum_{i=1}^{10} i P(i|y,x)\\)를 구하고 \\([-1,1]\\)로 정규화 해서 최종 score를 구한다. 이 score를 RM score를 대신해서 RL한다. (Appendix D)\n(메모) 논문에서 말하는 off-the-shelf나 general purpose LLM은 summarization, helpful, harmless 대화 생성에 특화 모델이 아니라는 것을 가정하는 듯. 이 논문에서는 summarization, helpful, harmless 대화 생성에 특화된 모델을 가지고 RLHF와 비교한다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#evaluation",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#evaluation",
    "title": "RLAIF vs RLHF Review",
    "section": "2.3 Evaluation",
    "text": "2.3 Evaluation\n\n3가지 메트릭을 도입 : AI Labeler Alignment, Win Rate, Harmless Rate\nAI Labeler Alignment : AI-labeled preferences를 binary representation으로 변환해서(e.g. [0.6, 0.4] -&gt; [1, 0]) human preferences와 동일하면 1, 아니면 0. \\[\nz_{\\text{acc}} = \\frac{1}{D} \\sum_{i=1}^{D} \\mathbb{1}\n[\\underset{j}{\\text{argmax}}\\,P_{i,j}^{\\text{AI}} = p_i^{H}]\n\\]\n\\(D\\) : preference dataset의 size, \\(P^{AI} \\in \\mathbb{R}^{D\\times2}\\) : matrix of soft AI preferences, \\(p^H \\in \\mathbb{R}^D\\) : vector of human preferences\nWin Rate : 2개의 policies(responses)의 품질 측정, 어느 것을 더 선호하는지 체크. human annotator가 선택.\nHarmless Rate : Win Rate와 동일한 방법으로 harmless 체크."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#datasets",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#datasets",
    "title": "RLAIF vs RLHF Review",
    "section": "3.1 Datasets",
    "text": "3.1 Datasets\n\nReddit TL;DR, OpenAI’s Human Preferences, Anthropic Helpful and Harmless Human Preferences에서 사용한 datasets을 사용. (Appendix C)\nStanford Human Preferences dataset은 의미있는 효과를 보여주지 않음. (Appendix J)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#llm-labeling",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#llm-labeling",
    "title": "RLAIF vs RLHF Review",
    "section": "3.2 LLM Labeling",
    "text": "3.2 LLM Labeling\n\nAI Labeling을 빨리하기 위해 각 데이터셋의 training split을 downsampling을 함. Summarization에서 human annotators가 높은 신뢰도로 선호도 체크한 것만 필터링. Downsampling과 Filtering 후 task별 3-4k만 남기고 AI labeling을 함.\nAI labeling으로 PaLM 2 Large를 사용. RL로 학습된 적이 없다고 함. (Appendix D)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#model-training",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#model-training",
    "title": "RLAIF vs RLHF Review",
    "section": "3.3 Model Training",
    "text": "3.3 Model Training\n\n모든 SFT 모델은 PaLM 2 Extra-Small(XS)로 초기화(baseline, checkpoint로 사용). Summarization은 Reddit TL;DR dataset으로 fine-tuning을 한 모델을 사용. 그 외의 tasks는 task-specific fine-tuning 말고 instruction-tuned 변형을 사용.\n모든 RM은 PaLM 2 XS checkpoints가 baseline. AI preference와 human preference label을 포함한 모든 training split으로 fine-tuning을 함. (Appendix G)\n최근엔 PPO를 많이 사용하지만, 이 논문에서는 여전히 효과적인 REINFORCE (Williams, 1992)의 변형을 사용했다. Policy, value model은 SFT model로 초기화. Summarization의 초기 state는 Reddit TL;DR dataset의 training split. Helpful, harmless tasks의 초기 state는 preference dataset의 training split. (Appendix E, Appendix H)\nSummarization에서는 RL-trained policies에 생성된 responses에 간단한 post-processing 적용.\nAppendix F"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#human-evaluation",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#human-evaluation",
    "title": "RLAIF vs RLHF Review",
    "section": "3.4 Human Evaluation",
    "text": "3.4 Human Evaluation\n - Test dataset으로 RLAIF, RLHF, SFT 각각 response 생성해서 누가 이겼는지 사람이 측정. (Appendix I)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#rlaif-vs-rlhf",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#rlaif-vs-rlhf",
    "title": "RLAIF vs RLHF Review",
    "section": "4.1 RLAIF vs RLHF",
    "text": "4.1 RLAIF vs RLHF\n\nRLAIF는 RLHF와 비슷하거나 더 나은 성능을 보임.\nRLAIF와 RLHF는 SFT보다 human evaluators가 더 선호했다.\nRLAIF는 Harmless task에서 가장 좋은 성능을 보임. \nLearning to summarize from human feedback 논문 처럼, RLAIF와 RLHF가 SFT에 비해 더 긴 responses를 생성했다. (메모 - DeepSeek-R1도 RL하니까 responses가 점점 길어졌다고 함) 길이가 human evaluation에게 편향을 일으킬 수 있지만, 길이를 제한한 사후 분석(post-hoc analysis)을 해봐도 RLAIF와 RLHF가 SFT를 이겼다. (Appendix J)\nHuman과 AI feedback을 결합해서 실험도 해봤지만 순수 Human feedback을 이기지 못했다. (Appendix K)\nRLAIF는 AI labeling으로 시간과 비용을 줄여주는 좋은 대체제다. (Appendix L)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#towards-self-improvement",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#towards-self-improvement",
    "title": "RLAIF vs RLHF Review",
    "section": "4.2 Towards Self-Improvement",
    "text": "4.2 Towards Self-Improvement\n\n4.1은 PaLM 2 L로 AI Labeling. Self-Improvement를 실험하기 위해 동일한 크기의 모델(PaLM 2 XS)로도 실험 (Same-size RLAIF). 나머지 세팅은 4.1과 같음. 이 실험도 역시 human annotators가 필요한 SFT를 크게 이김.\n하지면 엄밀한 self-improvement는 아닌데, AI labeler로 instruction-tuned PaLM 2 XS를 사용했고, 초기 policy model은 Reddit TL;DR summarization으로 fine-tuning을 했기 때문이다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#d-rlaif",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#d-rlaif",
    "title": "RLAIF vs RLHF Review",
    "section": "4.3 D-RLAIF",
    "text": "4.3 D-RLAIF\n\nd-RLAIF를 실험하기위해 instruction-tuned PaLM2 XS를 AI labeler로 사용.\nSummarization에서 D-RLAIF는 모델 크기를 동일하에 RLAIF를 60%로 이김. 이 부분은 AI labeler에게 preferences를 직접 물어 보는 점과 RLAIF의 RM에 preferences를 distilling하는 것이 성능이 안나오는(staleness) 것으로 추측.\nHelpful dialogue 생성에서는 SFT를 66%로 이겼는데, 정확히 같은 model checkpoint를 사용했기 때문에 LLM self-improvement의 엄격한 예시가 된다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#prompting-techniques",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#prompting-techniques",
    "title": "RLAIF vs RLHF Review",
    "section": "4.4 Prompting Techniques",
    "text": "4.4 Prompting Techniques\n\n여기에 효과가 있다는 것은 AI labeler alignment가 좋아졌다는 뜻.\n(Table2) CoT reasoning을 이끌어 내는 것은 효과가 있는 편. few-shot과 자세한 preambles(서문)은 케이스 마다 다름.\n자세한 서문은 summarization에서 효과가 있었다(그 외 X). 그 이유는 summarization이 좀 더 복잡한 task이기 때문인 것으로 추측.\nCoT는 일반적으로 도움이 됨. Helpful, harmless 대화 생성에서는 “Base” preamble에서만 효과있었다.\n놀랍게도, in-context learning(few-shot prompts)은 harmless 대화 생성에만 효과가 있었다. 이에 대해서는 off-the-shelf AI labeler가 summarization과 helpful 대화 생성에 대해 이미 잘 알고 있어서 few-shot prompts가 방해된 것이라고 추측.\nSummarization에서 LLM labeler의 절대적인 성능(absolute terms)을 측정하기 위해 human annotator 간의 동의도를 비교. Stiennon et al. (2020)에서 Open AI의 동의율은 73~77%인데 off-the-shelf LLM은 78%로 절대적인 성능도 높다.\nSelf-consistency를 실험하기 위해 temperature T &gt; 0으로 CoT rationales를 샘플링 했지만 AI labeler alignment가 떨어졌다. (Appendix M)\n높은 AI labeler alignment가 RLAIF policies의 성능을 올린다. 엄격한 실험 필요. (Appendix N)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#size-of-llm-labeler",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#size-of-llm-labeler",
    "title": "RLAIF vs RLHF Review",
    "section": "4.5 Size of LLM Labeler",
    "text": "4.5 Size of LLM Labeler\n\n모델 크기가 클수록 성능이 좋다. 작으면 성능이 안좋은 이유 중 하나는 2.1.1의 position bias가 있다. (Appendix B)"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#a.-rlhf-preliminaries",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#a.-rlhf-preliminaries",
    "title": "RLAIF vs RLHF Review",
    "section": "A. RLHF Preliminaries",
    "text": "A. RLHF Preliminaries\n\nA.1 Supervised Fine-tuning\n\n사전학습된 LLM은 downstream task를 위해 고품질의 labeled dataset으로 fine-tuing을 하는 것을 SFT라 한다. SFT 모델은 \\(\\pi^{\\text{SFT}}\\) (=policy)라 표현.\n\n\n\nA.2 Reward Modeling\n\ninput \\(x\\)로 하나 이상의 모델로 responses 쌍 \\((y_1,y_2) \\sim \\pi\\)을 구한다, \\(\\pi\\)는 SFT 모델이다. input과 responses는 annotators에게 어떤 기준으로 뭐를 더 선호하는지 매긴다. 그 결과 dataset of triplets \\(\\mathcal{D} = \\{(x,y_w,y_l)\\}\\)가 만들어지고, \\(y_w\\)는 선호, \\(y_l\\)은 비선호. Reward Model(RM) \\(r_\\phi\\)은 다음 loss를 감소하도록 학습된다. \\(\\sigma\\)는 sigmoid 함수 \\[\n\\mathcal{L}_r(\\phi) = \\underset{(x,y_w,y_l) \\sim \\mathcal{D}}{-\\mathbb{E}}\n\\left[\\text{log} \\, \\sigma(r_\\phi(x,y_w) - r_\\phi(x,y_l)) \\right]\n\\]\n위에서 만든 RM으로 모델의 responses에 rewards를 부여. 이 rewards로 RLAIF policy model을 RL한다.\n\n\n\nA.3. Reinforcement Learning\n\n\\(\\pi_{\\theta}^{\\text{RL}}\\)은 SFT 모델의 weights로 초기화하고 RM으로 부터 받은 reward를 최대화하는 RL을 한다.\nSFT policy \\(\\pi^{\\text{SFT}}\\)로 많이 멀어지는 것을 막기위해 KL divergence term \\(D_{\\text{KL}}\\)를 도입해서 \\(\\pi_{\\theta}^{\\text{RL}}\\)에 penalty를 줄 수 있다. (hyperparameter \\(\\beta\\)). 이 KL Loss는 “reward hacking”으로 불리는 저품질이거나 부자연스러운 언어인데 높은 reward를 받는 것을 방지한다.\nLoss function. \\(\\beta\\)는 0, 1사이. \\[\nJ(\\theta) = \\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot | x)} \\Big[ (1 - \\beta) r_{\\phi}(y | x) - \\beta D_{KL} \\big( \\pi^{RL}_{\\theta} (y | x) \\,||\\, \\pi^{SFT} (y | x) \\big) \\Big]\n\\]"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#b.-position-bias-in-llm-labelers",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#b.-position-bias-in-llm-labelers",
    "title": "RLAIF vs RLHF Review",
    "section": "B. Position Bias in LLM Labelers",
    "text": "B. Position Bias in LLM Labelers\n\n같은 위치에 있는 text 측정 방법은 후보 A,B 순서, 후보 B,A 순서로 각각 inference 했을때 같은 위치에 있는 것을 선호하는 것으로 측정.\n작은 모델일 수록 Position Bias가 더 prevalent(만연한)하다. PaLM 2 L(18%), S(21%), XS(56%)가 같은 위치에 있는 text를 더 선호했다.\nPaLM 2 L의 경우 18%의 같은 위치에 선호하는 케이스 중 94%가 앞에 나온 후보를 선호했고, 그 외 작은 모델은 뒤에 나온 후보를 선호(각각 91%, 99%)했다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#c.-dataset-details",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#c.-dataset-details",
    "title": "RLAIF vs RLHF Review",
    "section": "C. Dataset Details",
    "text": "C. Dataset Details\n\nsummarization : 고품질을 위해 필터링한 Reddit TL;DR dataset을 사용. 123k posts, 5%로 valid set. 추가로 OpenAI의 Reddit TL;DR dataset으로 부터 만들어진 human preference dataset을 사용. 92k pairwase 비교셋 존재.\nhelpful and harmless : Anthripic의 Helpful and Harmless preference dataset. 이 데이터는 human과 AI 간의 대화 및 선호도가 포함."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#d.-llm-labeling-details",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#d.-llm-labeling-details",
    "title": "RLAIF vs RLHF Review",
    "section": "D. LLM Labeling Details",
    "text": "D. LLM Labeling Details\n\nLLM labeling 설정\n\nmaximum input context length 4096\nCoT 생성 maximum decoding length 512, temperature T = 0.0.\nself-consistency 실험 : temperature T = 0.3 ~ 1.0, top-K = 40\n\nsummarization prompt\n\nYou are an expert summary rater. Given a TEXT (completed with a SUBREDDIT and a TITLE) and a SUMMARY, your role is to provide a SCORE from 1 to 10 that rates the quality of the SUMMARY given the TEXT, with 1 being awful and 10 being a perfect SUMMARY\", input Reddit Post, \"SUMMARY:\", summary to score, \"SCORE:\n\nhelpful task generation prompt\n\nYou are an expert rater of helpful and honest Assistant responses. Your role is to provide a SCORE from 1 to 10 that rates the helpfulness and honesty of the RESPONSE for a given CONTEXT. Where SCORE of 1 refers to useless and dishonest RESPONSE and a SCORE of 10 refers to a perfectly helpful and honest RESPONSE.\n\n위의 프롬프트 뒤에 conversation history and a response 추가\nPaLM2 사용"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#e.-reinforce-for-language-models",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#e.-reinforce-for-language-models",
    "title": "RLAIF vs RLHF Review",
    "section": "E. REINFORCE for Language Models",
    "text": "E. REINFORCE for Language Models"
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#f.-model-training-details",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#f.-model-training-details",
    "title": "RLAIF vs RLHF Review",
    "section": "F. Model Training Details",
    "text": "F. Model Training Details\n\nsummarization을 위한 SFT model은 Reddit TL;DR dataset으로 학습. batch size 128로 한 epoch. Adafactor Optimizer \\(\\gamma = 10^{-5}\\). max input length 1024, max output length 128. helpful and harmless task에선 instruction tuned PaLM 2 XS를 SFT model로 사용.\nRM은 training loss와 accuracy curve가 평탄(plateau)해 질 때까지 학습. 보통 2-3 epochs에 나타남. Adafactor Optimizer \\(\\gamma = 10^{-5}\\), batch size 128 for summerization, 32 for others. max input length 1152, 1024 context tokens, 128 response. 정확도는 Appendix G.\nsummarization에서 AI feedback RM은 SFT model로 초기화. human feedback RM은 PaLM 2 XS로 초기화. (SFT 모델로 초기화하면 정확도가 낮았음). helpful and harmless dialog에서는 AI, human feedback RM 둘 다 PaLM 2 XS로 초기화\nRL 에선 SFT model을 시작 policy로 사용. \\(T = 0.9\\)로 샘플링. batch size 128, LR \\(\\gamma = 10^{-5}\\), 8 epochs. KL diverge loss에서 \\(\\beta = 0.05\\)\n최종 RL policy 선정은 valid prompts에서 높은 rewards를 받은 4개의 checkpoint 중 off-the-shelf LLM이 선정. RL checkpoints의 답변과 SFT policy 답변 중 누가 이겼는지 off-the-shelf가 판단. 또 여러 예제를 직접 보면서 checkpoint 선정."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#g.-reward-model-accuracy",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#g.-reward-model-accuracy",
    "title": "RLAIF vs RLHF Review",
    "section": "G. Reward Model Accuracy",
    "text": "G. Reward Model Accuracy\n\n학습된 RM의 정확도를 측정하기 위해 Pairwise Accuracy 도입. 입력과 한 쌍의 후보 응답이 주어졌을때 인간 레이블에 더 높은 점수를 부여하면 1, 아니면 0.\n인간 피드백으로 학습한 RM이 더 높은 정확도를 보이는데 자연스러움. 하지만 성능은 RLAIF가 RLHF과 비슷하거나 좋다는 점.\nAppendix N에서는 RM 정확도가 반드시 인간 선호도로 이어지지 않는다는 것을 보임.\n그래서 RM 정확도가 반드시 RM의 효과를 반영하고 있지 않음. RM이 효과가 있는지 확인하려면 인간 평가를 해야한다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#h.-post-rl-response-formatting",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#h.-post-rl-response-formatting",
    "title": "RLAIF vs RLHF Review",
    "section": "H. Post-RL Response Formatting",
    "text": "H. Post-RL Response Formatting\n\n요약에서 응답 끝부분에 불필요한 마침표나 공백이 포함. (reward hacking 가능성)\n불필요한 기호는 프로그램적으로 제거."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#i.-human-evaluation-details",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#i.-human-evaluation-details",
    "title": "RLAIF vs RLHF Review",
    "section": "I. Human Evaluation Details",
    "text": "I. Human Evaluation Details\n\nHuman 평가를 위해 2,000개의 context-responses 쌍 생성. response는 SFT, RLAIF, RLHF policy가 각각 생성해서 총 6,000개. 평가자는 3명으로 총 18,000개 context-response-rating 튜플 생성.\n평가자들 사이에 일치도 파악을 위해 Kendall’s Coefficient of Concordance W 도입. 0은 불일치, 1은 일치. W 통계는 0.6 ~ 0.7로 적당히 일치함."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#j.-controlling-for-response-length",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#j.-controlling-for-response-length",
    "title": "RLAIF vs RLHF Review",
    "section": "J. Controlling for Response Length",
    "text": "J. Controlling for Response Length\n\n응답 길이가 평가에 영향을 줄 수 있기 때문에 통제를 하고 승률 측정.\n길이 보정을 구하는 방법은 policy A, B 각각의 요약 길이의 비율을 입력으로 A가 더 선호 되었는지의 이진 분류를 학습 후 예측.\nRLAIF와 RLHF가 SFT 대비 보정된 승률은 59%, 61%. 다른 작업에서도 비슷한 결과가 나온다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#k.-combining-human-and-ai-feedback",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#k.-combining-human-and-ai-feedback",
    "title": "RLAIF vs RLHF Review",
    "section": "K. Combining Human and AI Feedback",
    "text": "K. Combining Human and AI Feedback\n\nRLHF + RLAIF는 RLHF 단독으로 학습하는 것을 이기지 못했다.\nRLAIF로 웜업하고 RLHF를 위해 refine 작업을 하는 curriculum learning을 적용해볼 수 있다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#l.-cost-of-llm-vs.-human-labeling",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#l.-cost-of-llm-vs.-human-labeling",
    "title": "RLAIF vs RLHF Review",
    "section": "L. Cost of LLM vs. Human Labeling",
    "text": "L. Cost of LLM vs. Human Labeling\n\nLLM을 data annotator로 사용하는 것은 human annotators 보다 10배 싸다. 평균적으로 LLM은 예제당 $0.06 USD, human은 $0.67 USD가 든다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#m.-self-consistency",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#m.-self-consistency",
    "title": "RLAIF vs RLHF Review",
    "section": "M. Self-Consistency",
    "text": "M. Self-Consistency\n\nself-consistency test를 위해 T &gt; 0을 실험했지만, T가 증가할수록 AI Labeler alignment는 떨어졌다. CoT rationales를 직접 확인해도 공통된 패턴을 확인할 수 없었다."
  },
  {
    "objectID": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#n.-end-to-end-sensitivity-to-ai-labeler-alignment",
    "href": "posts/rlaif_vs_rlhf/rlaif_vs_rlhf.html#n.-end-to-end-sensitivity-to-ai-labeler-alignment",
    "title": "RLAIF vs RLHF Review",
    "section": "N. End-to-end Sensitivity to AI Labeler Alignment",
    "text": "N. End-to-end Sensitivity to AI Labeler Alignment\n\nAI Labeler Alignment의 민감도를 평가하기 위해 Base 0-shot, Detailed CoT 0-shot으로 AI Labeling을 하고 RLAIF 학습을 진행. AI Labeler Alignment는 각각 76.1%, 78.0%.\n직접 비교 했을때 평가자(인간)은 59%의 비율로 Detailed CoT 0-shot을 선호. 이는 AI Labeler Alignment의 작은 향상이 최종 RL policy를 크게 개선 시킬수도 있음을 알려줌.\n하지만 연구는 제한적이라 추가 실험이 필요."
  }
]